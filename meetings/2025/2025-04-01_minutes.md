# SING 

# Participants

* Simone Onofri (W3C)
* Patrick Shaller
* Gabriele Bellini
* Jaromil
* Amir Sharif
* Veronica Cristiano
* Tom Jones
* Samuel Schlesinger
* Manu Sporny (Digital Bazaar)
* Joe Andrieu


# Notes

## Introduction

Simone: SING call for 2025 - agenda is new participants, security reviews, threat models, documents that need volunteers
Simone: We have slack channels for those needing reviews, remember the W3C Code of Conduct, we're here to make the web secure (not to blame people for security issues)
Gabriele: Hi Gabriele from Dyne, use W3C standards, implement them, happy to be here.


## Security Review: Verifiable Credentials Data Integrity

Veronica: We've been reviewing VCDI, there are notes in the document. How are we going about it? Looking at DI, ECDSA, EdDSA, looking at all of them. Opinion - security considerations are well written, covers lots of cases and possible attacks. 

Veronica: About parameters, why just consider two security levels -- 256 and 384 -- why not 224 or 256 bit security? In ECDSA and EdDSA, it's the same -- in ECDSA, P-256 and P-384 are mentioned -- why are P-512 or Ed448 not considered? The current ones are secure and are NIST standards, in terms of security, there are curves that are more secure.

Manu: That's a good question, and the main reason is that VCDI is optimized for the next five years to avoid downgrade attacks -- attackers use downgrade attacks, so weakest one is often used. This is also supported by Hardware Security Modules (HSMs)  and commodity hardware -- many do not support P-521. Also, based on a risk assessment of the time needed to break encryption, Another point is interoperability. The compute needed to brute-froce break P-256 and P-384 is still very much out of reach and it's unlikey that something comes along that will break P-384 but not also break P-521.

Jaromil: Security of parameters, discussion is not new for many of us -- some arguments are sensitive in Europe on implementation of VCs for EIDAS -- the HSM argument is weak because HSM in phones are not really certified as "real HSMs", and since not even present in all the phones they would imply public sector credentials to create double-standards for citizens who can afford high end phones with HSM. I don't want to debunk that now -- my point, what Veronica raises is something worth checking in deep -- also what kind of cryptography deployed in VCs is analog to more resistant implementations: not just P-256, which can be used also in Zk schemes, but also in post-quantum for longer-term credentials. It depends on threat model, could be recommendations for certain use of parameters or cryptographic algortihsm instead of weaks ones -- ones we've talked about have been around for long time, industry has been resistant to upgrade, getting close to post-quantum mechanisms and could be upgraded. Could support different ones. How to fit into standards algorithms for post-quantum... FIPS 203 / FIPS 204, it's rather easy to integrate -- mentioning the possible alternatives without going deep into discussion on whether impleemnted or not, would help first movers to be more confident on stronger parameters/algorithms, given they have same functional flow.

Veronica: aren't these different topics? 

Manu: agrees this is perhaps out of scope now. Recharting the work on post-quantum cryptography is necessary and we are busy on it on other fronts.

Veronica: Cryptographic layering employs multiple different algorithms at same time to survive failures of one or more cryptographic algorithms without losing the protection. Choose to digitally sign some data using ECDSA and Falcon (post quantum). If cryptographic protection for ECDSA fails, the data is protected with Falcon still -- that is the meaning of cryptographic layering. This mechanism is useful for this historical moment when hybrid post-quantum solutions are recommended. Use of classical primitives and post-quantum primitives. Crypto layering is particularly useful for this reason -- for hybrid post-quantum solutions. Personally, don't see any other reason to use different classical algorithms to sign same data. Might be better to explain post-quantum topic. In addition to that, might be useful to point out differences on concatenation of signatures ECDSA + Falcon -- difference between (chain signatures) or two or more separate digital signatures of same data (set signatures). On one hand, signature of signature of data, on other hand two signatures of same data, which is different -- we have to explain the choice, could lead to implementation challenges -- size of data, structure, etc. Other party has to know structure of data they receive -- suggest explaining topic and pay attention to this concept.

Manu: the DI group discussed the pre-quantum and post-quantum, or whether we need a hybrid approach. I love to see the approach of this community. 

Veronica: yes, I agree that in pre-quantum and post-quantum hybrid schemes, set vs. chain doesn't make a difference, but we shoudl be clear about that in the spec -- explain the usage scenarios and where/how to use them.

Veronica: There are new anonymous credential constructions with ECDSA that provide unlinkability without changing issuer processes -- released by Google/Microsoft -- propose ZK-SNARK schemes into anonymous credentials. Could achieve unlinkability. Currently BBS has to be used, but is not NIST standard. We could use new ECDSA ZK SNARK to use create unlinkable signatures... we could have unlinkability with ECDSA in the future.

Manu: this is very interesting. I think there are some challenges not highlighted in the papers, e.g., type of cryptography; BBS is on CFRG and expected to be standardized soon. Also this mechanism is not approved by NIST. zkSNARK portion is not post-quantum resistant. It should be interesting to discuss it; the derivation is post-quantum resistant; the previous part, no. Another point is efficiency of Cryptographic cirtuit and very specific of he format. The process is interesting; we could potentially write something in the specification. Talking with NIST about stadardizing BBS, and working for ZKP PQC.

Sam: To clarify the post-quantum claim -- the proof system, that library can support post-quantum credentials of arbitrary type, but I don't think they're saying that ECDSA is post-quantum secure in any sense. The point is about the ZK Proof and not combined credential scheme.

Manu: I agree with Sam, it is not PQC in the sense people are saying: proof scheme vs original credential. Not suitable for achieving in EU ZKP PQC.

Veronica: Yes, I believe that's the whole review.

Simone: Thank you for the deep dive today. Let's crystalize discussion into concrete reviews and issues on Data Integrity specification.

## Security Review: other reviews

Simone: Continuing on other work items; WebAuthn, FedCM, big analysis on WebAuthn - notes on understanding from Level 1, and then FedCM group, request for review but also they used our new structure for security considerations sections, good to analyze. Please comment on the issue on Github and be assigned to review. Three new requests, please look at new specs - Web Neural Network API, Web App Scope extensions, and WebRTC API. New Charter for Distributed Tracing WG.

## Next call

Simone: Next call is 15 April, next on 29 April -- let us know if you want to have weekly meetings, otherwise, we'll use those dates. Open Wallet Foundation organizing conference on digital identities, break out session on day 2 for digital credentials. Breakout sessions on Digital Credentials API -- if interested in organizing threat modelling breakout, contact me and we can discuss.

## Threat Models: AI in the browser

Simone: Threat models - AI in the Browser done by Tom, want to cover now or next call?

Tom: Yes, whether or not the scope of the problem is correct. To clarify, the issue is whether or not the use of AI in the browser is secure or not -- what we should focus on is case where AI is conditioned as a result of passed input. If AI is conditioned on input, then it's not secure in browser -- skeptical about AI in browser in general, seems like Chrome is going to get AI that can't be poisoned into the browser. If AI can be poisoned, it might not be able to be made secure.

Manu: using open data, open model and don't allow the agents to learn to much during the interactions. The problem that if you drop 250k token, but poisoning can happen.

Tom: it is not about the people think now about AI, but also how other people have access to it.

Simone: yes, clarifying the scope would be useful -- need different models for AI. Recently talking, implementation w/ AI with user agents to be more generic than browsers, AI that you can ask "please do this for me" (agents), using your credentials and acting like you (masquerading).

Tom: Maybe genericize this to be "AI in any agent"?

Simone: Browser is specific type of agent. One threat model for specific thing is simpler to write down.

Tom: Ok, browser or any user agent that has access to individual data. Number of user agents might grow exponentially, maybe browsers become less interesting?

Joe: Maybe a diagram would help, Tom
